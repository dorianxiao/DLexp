{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PTB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorianxiao/DLexp/blob/master/Exp3%3A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/PTB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITWWSeb3Fpca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 挂载到Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DrJyc9OI5hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# 更改工作目录\n",
        "os.chdir(r'/content/gdrive/My Drive/mylab/Exp3')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQwNwghmLibm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入相关库\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rfYChn8I8Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据预处理\n",
        "with open('./ptb.train.txt','r') as f1,open('./ptb.valid.txt','r') as f2,open(\n",
        "    './ptb.test.txt','r') as f3:\n",
        "    seq_train=f1.read().replace('\\n','<eos>').split(' ')\n",
        "    seq_valid=f2.read().replace('\\n','<eos>').split(' ')\n",
        "    seq_test=f3.read().replace('\\n','<eos>').split(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOuXqmiCLdJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_train=list(filter(None,seq_train))\n",
        "seq_valid=list(filter(None,seq_valid))\n",
        "seq_test=list(filter(None,seq_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY-3doNnMQSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_train=len(seq_train)\n",
        "size_valid=len(seq_valid)\n",
        "size_test=len(seq_test)\n",
        "print('size_train {}, size_valid {}, size_test {}'.format(\n",
        "    size_train, size_valid, size_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIBmuPvMX-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(seq_train[:10])\n",
        "print(seq_valid[:10])\n",
        "print(seq_test[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThArBYqRMtYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 建立词汇表\n",
        "\n",
        "vocab_train=set(seq_train)\n",
        "vocab_valid=set(seq_valid)\n",
        "vocab_test=set(seq_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtossyAvMw6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert vocab_valid.issubset(vocab_train)\n",
        "assert vocab_test.issubset(vocab_train)\n",
        "print('vocab_train {}, vocab_valid {}, vocab_test {}'.format(\n",
        "    len(vocab_train),len(vocab_valid),len(vocab_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OZhG7axM1iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 先将词汇表进行排序\n",
        "vocab_train=sorted(vocab_train)\n",
        "\n",
        "# 建立word2id和id2word字典\n",
        "word2id={w:i for i,w in enumerate(vocab_train)}\n",
        "id2word={i:w for i,w in enumerate(vocab_train)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjFG6nUONERS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids_train=np.array([word2id[word] for word in seq_train],copy=False,order='C')\n",
        "ids_valid=np.array([word2id[word] for word in seq_valid],copy=False,order='C')\n",
        "ids_test=np.array([word2id[word] for word in seq_test],copy=False,order='C')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SyAtV5UNOT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_labels(data_array,batch_size,seq_len,batch_first=True):\n",
        "  \"\"\"Splits the sequential data into batch_size number of sub_sequences and \n",
        "  folds them into the requisite shape. This procedure is applied to the data to \n",
        "  derive the features array. This procedure is repeated to derive the labels \n",
        "  array also, except in this case the data is shifted by one time step. \n",
        "  Returns a named tuple of features and labels.\n",
        "  \n",
        "  Args:\n",
        "    data_array: np.int64 1-d numpy array of shape (size,)\n",
        "    batch_size: int;\n",
        "    seq_len: int; length of the rnn layer\n",
        "    batch_first: boolean; the returned numpy arrays will be of shape \n",
        "      (batch_size*steps, seq_len) if True and \n",
        "      (seq_len*steps, batch_size) if False\n",
        "      \n",
        "  Returns:\n",
        "    named tuple of features and labels, features and labels are np.int64 2-d \n",
        "      numpy arrays of shape (batch_size*steps, seq_len) if batch_first is True \n",
        "      and (seq_len*steps, batch_size) if batch_first is False\n",
        "    steps: int; number of mini batches in an epoch\n",
        "      \n",
        "  Raises:\n",
        "    ValueError: If input data_array is not 1-d\n",
        "\n",
        "  \"\"\"\n",
        "  if len(data_array.shape) != 1:\n",
        "    raise ValueError('Expected 1-d data array, '\n",
        "                     'instead data array shape is {} '.format(data_array.shape))\n",
        "  \n",
        "  def fold(used_array):\n",
        "    shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')\n",
        "    \n",
        "    if batch_first:\n",
        "      return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\n",
        "    else:\n",
        "      return np.transpose(shaped_array)\n",
        "\n",
        "  steps=(data_array.shape[0]-1)//(batch_size*seq_len)\n",
        "  used=batch_size*seq_len*steps\n",
        "  \n",
        "  features=fold(data_array[:used])\n",
        "  labels=fold(data_array[1:used+1])\n",
        "  \n",
        "  Data=collections.namedtuple('Data',['features','labels'])\n",
        "  return Data(features=features,labels=labels),steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyItPTj8NVkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 设置超参数\n",
        "\n",
        "batch_size=20\n",
        "seq_len=20\n",
        "clip_norm=5\n",
        "learning_rate=1.\n",
        "decay=0.5\n",
        "epochs=13\n",
        "epochs_no_decay=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzxU5wlUNgr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train,steps_train=features_labels(\n",
        "    ids_train,batch_size,seq_len,batch_first=False)\n",
        "data_valid,steps_valid=features_labels(\n",
        "    ids_valid,batch_size,seq_len,batch_first=False)\n",
        "data_test,steps_test=features_labels(\n",
        "    ids_test,batch_size,seq_len,batch_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMHZLiQ9Nm16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建数据集\n",
        "\n",
        "dataset_train=tf.data.Dataset.from_tensor_slices(data_train).batch(seq_len,\n",
        "    drop_remainder=True)\n",
        "dataset_valid=tf.data.Dataset.from_tensor_slices(data_valid).batch(seq_len,\n",
        "    drop_remainder=True)\n",
        "dataset_test=tf.data.Dataset.from_tensor_slices(data_test).batch(seq_len,\n",
        "    drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um9vboDDNq3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建PTB对象\n",
        "\n",
        "class PTBModel(object):\n",
        "  \"\"\"\n",
        "  Ref: \n",
        "    Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, \n",
        "    \"Recurrent Neural Network Regularization\", ICLR 2015\n",
        "    \n",
        "  Imports:\n",
        "    tf\n",
        "    tf.nn\n",
        "    tf.math\n",
        "    tf.contrib.cudnn_rnn.CudnnLSTM - requires GPU\n",
        "    tf.layers.Dense\n",
        "    tf.train.GradientDescentOptimizer\n",
        "    tf.contrib.eager\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, \n",
        "               vocab_size, \n",
        "               embedding_dim, \n",
        "               hidden_dim, \n",
        "               num_layers, \n",
        "               initializer, \n",
        "               dropout_ratio):\n",
        "    \"\"\"Initalizes a model instance. Configures network layers.\n",
        "    \n",
        "    Args:\n",
        "      vocab_size: int;\n",
        "      embedding_dim: int; \n",
        "      hidden_dim: int; hidden state size, cell state size\n",
        "      num_layers: int; number of layers\n",
        "      initializer: initializer object for initializing kernels and biases\n",
        "      dropout_ratio: float; drop out ratio\n",
        "\n",
        "    Returns:\n",
        "      a model instance\n",
        "\n",
        "    Raises:\n",
        "      \n",
        "    \"\"\"\n",
        "    self._embedding=tf.get_variable(\n",
        "        'embedding',\n",
        "        shape=(vocab_size, embedding_dim),\n",
        "        initializer=initializer,\n",
        "        trainable=True)\n",
        "    \n",
        "    #crashes when input_mode='skip_input' or 'auto_select'\n",
        "    self._rnn=tf.contrib.cudnn_rnn.CudnnLSTM(\n",
        "        num_layers=num_layers, \n",
        "        num_units=hidden_dim, \n",
        "        dropout=dropout_ratio,\n",
        "        kernel_initializer=initializer,\n",
        "        input_mode='linear_input')\n",
        "\n",
        "    self._dense=tf.layers.Dense(\n",
        "        units=vocab_size,        \n",
        "        kernel_initializer=initializer)\n",
        "        \n",
        "    self._training=False\n",
        "    self._state=None\n",
        "    \n",
        "  def _call(self,inp):\n",
        "    \"\"\"Executes a forward prop through the network.\n",
        "    \n",
        "    Args:\n",
        "      inp: input (features) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
        "      \n",
        "    Returns:\n",
        "      y: output logits tf.float32 3-d tensor of shape (seq_len,batch_size,\n",
        "        vocab_size)\n",
        "        \n",
        "    Raises:\n",
        "      \n",
        "    \"\"\"\n",
        "    \n",
        "    y=tf.nn.embedding_lookup(self._embedding, inp)     \n",
        "    y,self._state=self._rnn(y,initial_state=self._state,training=self._training)\n",
        "    y=self._dense(y)\n",
        "    \n",
        "    return y\n",
        "  \n",
        "  def _loss(self, inp, target):\n",
        "    \"\"\"Calls forward prop, gets logits, computes softmax, and calculates total\n",
        "    cross-entropy loss for a mini batch.\n",
        "    \n",
        "    Args:\n",
        "      inp: input (features) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
        "      target: target (labels) tf.int64 2-d tensor of shape (seq_len,batch_size)\n",
        "      \n",
        "    Returns:\n",
        "      loss: tf.float32 0-d tensor\n",
        "        \n",
        "    Raises:\n",
        "      \n",
        "    \"\"\"\n",
        "    \n",
        "    y=self._call(inp)\n",
        "    return tf.math.reduce_sum(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target,logits=y))\n",
        "  \n",
        "  def train(self, iterator, clip_norm, optimizer):\n",
        "    \"\"\"Runs an epoch of training.\n",
        "        \n",
        "    Args:\n",
        "      iterator: iterator object that yields (input,target) tuples where input  \n",
        "        and target both are tf.int64 2-d tensors of shape (seq_len,batch_size)\n",
        "      clip_norm: float; maximum global norm for clipping gradients\n",
        "      optimizer: optimizer object for applying gradients\n",
        "      \n",
        "    Returns:\n",
        "      A python list of losses, one loss (float) per step\n",
        "      \n",
        "    Raises:\n",
        "    \n",
        "    \"\"\"\n",
        "    self._training=True\n",
        "    self._state=None\n",
        "    losses=[]\n",
        "    \n",
        "    while True:\n",
        "      try:\n",
        "        inp,target=iterator.get_next()\n",
        "        loss,grad_var=tf.contrib.eager.implicit_value_and_gradients(self._loss)(\n",
        "            inp,target)\n",
        "        losses.append(loss.numpy())\n",
        "        \n",
        "        gradients,variables=zip(*grad_var)\n",
        "        clipped, global_norm=tf.clip_by_global_norm(gradients, clip_norm)\n",
        "        grad_var=zip(clipped, variables)\n",
        "        optimizer.apply_gradients(grad_var)\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "        \n",
        "    return losses\n",
        "        \n",
        "  def evaluate(self, iterator):\n",
        "    \"\"\"Runs an epoch of evaluation.\n",
        "        \n",
        "    Args:\n",
        "      iterator: iterator object that yields (input,target) tuples where input \n",
        "        and target both are tf.int64 2-d tensors of shape (seq_len,batch_size)\n",
        "      \n",
        "    Returns:\n",
        "      A python list of losses, one loss (float) per step\n",
        "      \n",
        "    Raises:\n",
        "    \n",
        "    \"\"\"\n",
        "    self._training=False\n",
        "    self._state=None\n",
        "    losses=[]\n",
        "    \n",
        "    while True:\n",
        "      try:\n",
        "        inp,target=iterator.get_next()       \n",
        "        losses.append(self._loss(inp,target).numpy())\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "        \n",
        "    return losses\n",
        "\n",
        "  @classmethod\n",
        "  def instance(cls, model_type='small', vocab_size=10000):\n",
        "    \"\"\"Returns a model instance.\n",
        "    \n",
        "    Args:\n",
        "      model_type: string; 'small', 'medium' or 'large'; only 'small' is\n",
        "        implemented presently\n",
        "      vocab_size: int;\n",
        "\n",
        "    Returns:\n",
        "      a model instance\n",
        "\n",
        "    Raises:\n",
        "      \n",
        "    \"\"\"\n",
        "    return {\n",
        "        'small':PTBModel(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=200,\n",
        "            hidden_dim=200,\n",
        "            num_layers=2,\n",
        "            initializer=tf.random_uniform_initializer(minval=-0.1,maxval=0.1),\n",
        "            dropout_ratio=0.),\n",
        "        'medium':None,\n",
        "        'large':None\n",
        "        }.get(model_type,None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qolTXXCfN5VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建模型\n",
        "\n",
        "model=PTBModel.instance(model_type='small',vocab_size=len(word2id))\n",
        "lr=tf.contrib.eager.Variable(initial_value=learning_rate, trainable=False)\n",
        "optimizer=tf.train.GradientDescentOptimizer(lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8brQFLR5OGLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time=time.time()\n",
        "print('date {}'.format(datetime.datetime.now()))\n",
        "print('device {}'.format(tf.test.gpu_device_name()))\n",
        "print('TensorFlow vers. {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Ke9XKoObbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perplexity_train=[]\n",
        "perplexity_valid=[]\n",
        "\n",
        "print('\\n'+' '*24+'TRAINING'+'\\n'+\n",
        "      'time'+' '*6+\n",
        "      'epochs'+' '*9+\n",
        "      'loss'+' '*15+\n",
        "      'perplexity'+'\\n'+\n",
        "      ' '*20+\n",
        "      'train'+' '*4+\n",
        "      'valid'+' '*7+\n",
        "      'train'+' '*7+\n",
        "      'valid'+'\\n'+\n",
        "      '======'+' '*4+\n",
        "      '======'+' '*4+\n",
        "      '====='+' '*4+\n",
        "      '====='+' '*7+\n",
        "      '====='+' '*7+\n",
        "      '=====')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  iter_train=dataset_train.make_one_shot_iterator()\n",
        "  losses_train=model.train(iter_train,clip_norm,optimizer)\n",
        "  assert len(losses_train)==steps_train\n",
        "  loss_train_avg=sum(losses_train)/(len(losses_train)*seq_len*batch_size)\n",
        "  perplexity_train+=[(epoch+(step+1)/len(losses_train),math.exp(loss/(\n",
        "      seq_len*batch_size))) for step,loss in enumerate(losses_train)]\n",
        "  \n",
        "  iter_valid=dataset_valid.make_one_shot_iterator()\n",
        "  losses_valid=model.evaluate(iter_valid)\n",
        "  assert len(losses_valid)==steps_valid\n",
        "  loss_valid_avg=sum(losses_valid)/(len(losses_valid)*seq_len*batch_size)\n",
        "  perplexity_valid.append((epoch+1,math.exp(loss_valid_avg)))\n",
        "  \n",
        "  if epoch>epochs_no_decay-2:\n",
        "    lr.assign(lr*decay)\n",
        "\n",
        "  print('{:}'.format(datetime.timedelta(seconds=round(time.time()-start_time))),\n",
        "        '{:5.2f}'.format(epoch+1),\n",
        "        '{:5.2f}'.format(loss_train_avg),\n",
        "        '{:5.2f}'.format(loss_valid_avg),\n",
        "        '{:8.2f}'.format(math.exp(loss_train_avg)),\n",
        "        '{:8.2f}'.format(perplexity_valid[-1][1]),sep=' '*4)\n",
        "  \n",
        "plt.plot([loss[0] for loss in perplexity_train],\n",
        "         [loss[1] for loss in perplexity_train],\n",
        "         linewidth=1,color='red',label='training')\n",
        "plt.plot([loss[0] for loss in perplexity_valid],\n",
        "         [loss[1] for loss in perplexity_valid],\n",
        "         linewidth=1,color='blue',label='validation', marker='o')\n",
        "plt.grid(True,which='both',axis='both')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.axis([0,14,0,500])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "iter_test=dataset_test.make_one_shot_iterator()\n",
        "losses_test=model.evaluate(iter_test)\n",
        "assert len(losses_test)==steps_test\n",
        "loss_test_avg=sum(losses_test)/(len(losses_test)*seq_len*batch_size)\n",
        "print(\"\\nLoss/perplexity on test set after {} epochs: {:8.2f}/{:8.2f}\\n\".format(\n",
        "    epoch+1,loss_test_avg,math.exp(loss_test_avg)))\n",
        "\n",
        "print(\"execution time - {}s\".format(\n",
        "    datetime.timedelta(seconds=round(time.time()-start_time))))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}