{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorianxiao/DLexp/blob/master/Exp4%3A%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/Text_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-zF6Bed_caB",
        "colab_type": "text"
      },
      "source": [
        "# 1. 导入相关库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4JnaYHftriZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import timedelta\n",
        "from collections import Counter\n",
        "import tensorflow.contrib.keras as kr\n",
        "import jieba as jb\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07DU_DjEBh2H",
        "colab_type": "code",
        "outputId": "00397606-b2d4-4595-9328-9306ccb9c751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 挂载到Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EVq4ksDHWSB",
        "colab_type": "code",
        "outputId": "a727cee8-cba8-4dce-ab93-68ec8e83439b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 更改当前工作目录\n",
        "\n",
        "os.chdir(r'/content/gdrive/My Drive/mylab/Exp4')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/mylab/Exp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msPR4UVX_ixX",
        "colab_type": "text"
      },
      "source": [
        "# 2. 数据预处理\n",
        "### cat_to_id()：分类类别以及id对应词典\n",
        "正对应0，负对应1 -> {pos:0, neg:1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sulhnfqR3e4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cat_to_id(classes=None):\n",
        "    \"\"\"\n",
        "    :param classes: 分类标签；默认为0--pos   1--neg\n",
        "    :return: {分类标签：id}\n",
        "    \"\"\"\n",
        "    if not classes:\n",
        "        classes = ['0', '1']\n",
        "    cat2id = {cat: idx for (idx, cat) in enumerate(classes)}\n",
        "    return classes, cat2id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3_XJ0-TBFe3",
        "colab_type": "text"
      },
      "source": [
        "### build_word2id(): 构建词汇表并存储，形如{word: id}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tb9Ola3BDYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_word2id(file):\n",
        "    \"\"\"\n",
        "    :param file: word2id保存地址\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    word2id = {'_PAD_': 0}\n",
        "    path = ['./Dataset/train.txt', './Dataset/validation.txt']\n",
        "    print(path)\n",
        "    for _path in path:\n",
        "        with open(_path, encoding='utf-8') as f:\n",
        "            for line in f.readlines():\n",
        "                sp = line.strip().split()\n",
        "                for word in sp[1:]:\n",
        "                    if word not in word2id.keys():\n",
        "                        word2id[word] = len(word2id)\n",
        "\n",
        "    with open(file, 'w', encoding='utf-8') as f:\n",
        "        for w in word2id:\n",
        "            f.write(w+'\\t')\n",
        "            f.write(str(word2id[w]))\n",
        "            f.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5EZSY3g1yhI",
        "colab_type": "code",
        "outputId": "114dfc99-152c-4921-f0c9-19350d221091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 构建词汇表\n",
        "#build_word2id('./Dataset/word_to_id.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./Dataset/train.txt', './Dataset/validation.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwV5bAghIzXe",
        "colab_type": "text"
      },
      "source": [
        "### 加载上述构建的词汇表"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrg-CSYGUQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word2id(path):\n",
        "    \"\"\"\n",
        "    :param path: word_to_id词汇表路径\n",
        "    :return: word_to_id:{word: id}\n",
        "    \"\"\"\n",
        "    word_to_id = {}\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split()\n",
        "            word = sp[0]\n",
        "            idx = int(sp[1])\n",
        "            if word not in word_to_id:\n",
        "                word_to_id[word] = idx\n",
        "    return word_to_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoVLg5MLI4km",
        "colab_type": "text"
      },
      "source": [
        "### 基于预训练好的word2vec构建训练语料中所含词语的word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEAn-C4zI2OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_word2vec(fname, word2id, save_to_path=None):\n",
        "    \"\"\"\n",
        "    :param fname: 预训练的word2vec.\n",
        "    :param word2id: 语料文本中包含的词汇集.\n",
        "    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地\n",
        "    :return: 语料文本中词汇集对应的word2vec向量{id: word2vec}.\n",
        "    \"\"\"\n",
        "    import gensim\n",
        "    n_words = max(word2id.values()) + 1\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
        "    word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))\n",
        "    for word in word2id.keys():\n",
        "        try:\n",
        "            word_vecs[word2id[word]] = model[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if save_to_path:\n",
        "        with open(save_to_path, 'w', encoding='utf-8') as f:\n",
        "            for vec in word_vecs:\n",
        "                vec = [str(w) for w in vec]\n",
        "                f.write(' '.join(vec))\n",
        "                f.write('\\n')\n",
        "    return word_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm20bSHm25_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 加载词汇表\n",
        "word2id = load_word2id('./Dataset/word_to_id.txt')\n",
        "\n",
        "# 构建word2wec\n",
        "w2v = build_word2vec('./Dataset/wiki_word2vec_50.bin', word2id, save_to_path='./Dataset/corpus_word2vec.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doI8VMfRI9LB",
        "colab_type": "text"
      },
      "source": [
        "### 加载上述构建的word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rOFL4RaI8Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_corpus_word2vec(path):\n",
        "    \"\"\"加载语料库word2vec词向量,相对wiki词向量相对较小\"\"\"\n",
        "    word2vec = []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            sp = [float(w) for w in line.strip().split()]\n",
        "            word2vec.append(sp)\n",
        "    return np.asarray(word2vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iJGsHI1JC_n",
        "colab_type": "text"
      },
      "source": [
        "### 加载语料库：train/dev/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_pEPdMSJCce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_corpus(path, word2id, max_sen_len=70):\n",
        "    \"\"\"\n",
        "    :param path: 样本语料库的文件\n",
        "    :return: 文本内容contents，以及分类标签labels(onehot形式)\n",
        "    \"\"\"\n",
        "    _, cat2id = cat_to_id()\n",
        "    contents, labels = [], []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split()\n",
        "            if len(sp):\n",
        "                label = sp[0]\n",
        "                content = [word2id.get(w, 0) for w in sp[1:]]\n",
        "                content = content[:max_sen_len]\n",
        "                if len(content) < max_sen_len:\n",
        "                    content += [word2id['_PAD_']] * (max_sen_len - len(content))\n",
        "                labels.append(label)\n",
        "                contents.append(content)\n",
        "    counter = Counter(labels)\n",
        "    print('总样本数为：%d' % (len(labels)))\n",
        "    print('各个类别样本数如下：')\n",
        "    for w in counter:\n",
        "        print(w, counter[w])\n",
        "\n",
        "    contents = np.asarray(contents)\n",
        "    labels = [cat2id[l] for l in labels]\n",
        "    labels = kr.utils.to_categorical(labels, len(cat2id))\n",
        "\n",
        "    return contents, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97sR2xM5JK9g",
        "colab_type": "text"
      },
      "source": [
        "### 生成批处理id序列"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EO3y75VJHhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def  batch_index(length, batch_size, is_shuffle=True):\n",
        "    \"\"\"\n",
        "    生成批处理样本序列id.\n",
        "    :param length: 样本总数\n",
        "    :param batch_size: 批处理大小\n",
        "    :param is_shuffle: 是否打乱样本顺序\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    index = [idx for idx in range(length)]\n",
        "    if is_shuffle:\n",
        "        np.random.shuffle(index)\n",
        "    for i in range(int(np.ceil(length / batch_size))):\n",
        "        yield index[i * batch_size:(i + 1) * batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgwB1iouJOWn",
        "colab_type": "text"
      },
      "source": [
        "# 3. Text-CNN模型的建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu4ObMx7JNxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CONFIG():\n",
        "    update_w2v = True           # 是否在训练中更新w2v\n",
        "    vocab_size = 59290          # 词汇量，与word2id中的词汇量一致\n",
        "    n_class = 2                 # 分类数：分别为pos和neg\n",
        "    max_sen_len = 75            # 句子最大长度\n",
        "    embedding_dim = 50          # 词向量维度\n",
        "    batch_size = 100            # 批处理尺寸\n",
        "    n_hidden = 256              # 隐藏层节点数\n",
        "    n_epoch = 10                # 训练迭代周期，即遍历整个训练样本的次数\n",
        "    opt = 'adam'                # 训练优化器：adam或者adadelta\n",
        "    learning_rate = 0.001       # 学习率；若opt=‘adadelta'，则不需要定义学习率\n",
        "    drop_keep_prob = 0.5        # dropout层，参数keep的比例\n",
        "    num_filters = 256           # 卷积层filter的数量\n",
        "    kernel_size = 4             # 卷积核的尺寸；nlp任务中通常选择2,3,4,5\n",
        "    print_per_batch = 100       # 训练过程中,每100词batch迭代，打印训练信息\n",
        "    save_dir = './checkpoints/' # 训练模型保存的地址\n",
        "    train_path = './Dataset/train.txt'\n",
        "    dev_path = './Dataset/validation.txt'\n",
        "    test_path = './Dataset/test.txt'\n",
        "    word2id_path = './Dataset/word_to_id.txt'\n",
        "    pre_word2vec_path = './Dataset/wiki_word2vec_50.bin'\n",
        "    corpus_word2vec_path = './Dataset/corpus_word2vec.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubZ7uQbtJZds",
        "colab_type": "text"
      },
      "source": [
        "### 定义时间函数，供计算模型迭代时间使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_6gKgHMJVva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_diff(start_time):\n",
        "    \"\"\"当前距初始时间已花费的时间\"\"\"\n",
        "    end_time = time.time()\n",
        "    diff = end_time - start_time\n",
        "    return timedelta(seconds=int(round(diff)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuLSLpEJeJm",
        "colab_type": "text"
      },
      "source": [
        "### 建立Text-CNN模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t1eZ9fgJcbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextCNN(object):\n",
        "    def __init__(self, config, embeddings=None):\n",
        "        self.update_w2v = config.update_w2v\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.n_class = config.n_class\n",
        "        self.max_sen_len= config.max_sen_len\n",
        "        self.embedding_dim = config.embedding_dim\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_filters = config.num_filters\n",
        "        self.kernel_size = config.kernel_size\n",
        "        self.n_hidden = config.n_hidden\n",
        "        self.n_epoch = config.n_epoch\n",
        "        self.opt = config.opt\n",
        "        self.learning_rate = config.learning_rate\n",
        "        self.drop_keep_prob = config.drop_keep_prob\n",
        "\n",
        "        self.x = tf.placeholder(tf.int32, [None, self.max_sen_len], name='x')\n",
        "        self.y = tf.placeholder(tf.int32, [None, self.n_class], name='y')\n",
        "        \n",
        "        if embeddings is not None:\n",
        "            self.word_embeddings = tf.Variable(embeddings, dtype=tf.float32, trainable=self.update_w2v)\n",
        "        else:\n",
        "            self.word_embeddings = tf.Variable(\n",
        "                tf.zeros([self.vocab_size, self.embedding_dim]),\n",
        "                dtype=tf.float32,\n",
        "                trainable=self.update_w2v)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def cnn(self):\n",
        "        \"\"\"\n",
        "        :param mode:默认为None，主要调节dropout操作对训练和预测带来的差异。\n",
        "        :return: 未经softmax变换的fully-connected输出结果\n",
        "        \"\"\"\n",
        "        inputs = self.add_embeddings()\n",
        "        with tf.name_scope(\"cnn\"):\n",
        "            # CNN layer\n",
        "            conv = tf.layers.conv1d(inputs, self.num_filters, self.kernel_size, name='conv')\n",
        "            # global max pooling layer\n",
        "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
        "            # dropout 卷积层后加dropout效果太差\n",
        "            # gmp = tf.contrib.layers.dropout(gmp, self.drop_keep_prob)\n",
        "\n",
        "        with tf.name_scope(\"score\"):\n",
        "            # fully-connected\n",
        "            fc = tf.layers.dense(gmp, self.n_hidden, name='fc1')\n",
        "            # dropout\n",
        "            fc = tf.contrib.layers.dropout(fc, self.drop_keep_prob)\n",
        "            # nonlinear\n",
        "            fc = tf.nn.relu(fc)\n",
        "            # fully-connected\n",
        "            pred = tf.layers.dense(fc, self.n_class, name='fc2')\n",
        "        return pred\n",
        "\n",
        "    def add_embeddings(self):\n",
        "        inputs = tf.nn.embedding_lookup(self.word_embeddings, self.x)\n",
        "        return inputs\n",
        "\n",
        "    def add_loss(self, pred):\n",
        "        cost = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.y)\n",
        "        cost = tf.reduce_mean(cost)\n",
        "        return cost\n",
        "\n",
        "    def add_optimizer(self, loss):\n",
        "        if self.opt == 'adadelta':\n",
        "            optimizer = tf.train.AdadeltaOptimizer(learning_rate=1.0, rho=0.95, epsilon=1e-6)\n",
        "        else:\n",
        "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        opt = optimizer.minimize(loss)\n",
        "        return opt\n",
        "\n",
        "    def add_accuracy(self, pred):\n",
        "        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        return accuracy\n",
        "\n",
        "    def get_batches(self, x, y=None, batch_size=100, is_shuffle=True):\n",
        "        for index in batch_index(len(x), batch_size, is_shuffle=is_shuffle):\n",
        "            n = len(index)\n",
        "            feed_dict = {\n",
        "                self.x: x[index]\n",
        "            }\n",
        "            if y is not None:\n",
        "                feed_dict[self.y] = y[index]\n",
        "            yield feed_dict, n\n",
        "\n",
        "    def build(self):\n",
        "        self.pred = self.cnn()\n",
        "        self.loss = self.add_loss(self.pred)\n",
        "        self.accuracy = self.add_accuracy(self.pred)\n",
        "        self.optimizer = self.add_optimizer(self.loss)\n",
        "\n",
        "    def train_on_batch(self, sess, feed):\n",
        "        _, _loss, _acc = sess.run([self.optimizer, self.loss, self.accuracy], feed_dict=feed)\n",
        "        return _loss, _acc\n",
        "\n",
        "    def test_on_batch(self, sess, feed):\n",
        "        _loss, _acc = sess.run([self.loss, self.accuracy], feed_dict=feed)\n",
        "        return _loss, _acc\n",
        "\n",
        "    def predict_on_batch(self, sess, feed, prob=True):\n",
        "        result = tf.argmax(self.pred, 1)\n",
        "        if prob:\n",
        "            result = tf.nn.softmax(logits=self.pred, dim=1)\n",
        "\n",
        "        res = sess.run(result, feed_dict=feed)\n",
        "        return res\n",
        "\n",
        "    def predict(self, sess, x, prob=False):\n",
        "        yhat = []\n",
        "        for _feed, _ in self.get_batches(x, batch_size=self.batch_size, is_shuffle=False):\n",
        "            _yhat = self.predict_on_batch(sess, _feed, prob)\n",
        "            yhat += _yhat.tolist()\n",
        "            # yhat.append(_yhat)\n",
        "        return np.array(yhat)\n",
        "\n",
        "    def evaluate(self, sess, x, y):\n",
        "        \"\"\"评估在某一数据集上的准确率和损失\"\"\"\n",
        "        num = len(x)\n",
        "        total_loss, total_acc = 0., 0.\n",
        "        for _feed, _n in self.get_batches(x, y, batch_size=self.batch_size):\n",
        "            loss, acc = self.test_on_batch(sess, _feed)\n",
        "            total_loss += loss * _n\n",
        "            total_acc += acc * _n\n",
        "        return total_loss / num, total_acc / num\n",
        "\n",
        "    def fit(self, sess, x_train, y_train, x_dev, y_dev, save_dir=None, print_per_batch=100):\n",
        "        saver = tf.train.Saver()\n",
        "        if save_dir:\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        print('训练和验证...')\n",
        "        start_time = time.time()\n",
        "        total_batch = 0 # 总批次\n",
        "        best_acc_dev = 0.0  # 最佳验证集准确率\n",
        "        last_improved = 0   # 记录上次提升批次\n",
        "        require_improvement = 500  # 如果超过500轮模型效果未提升，提前结束训练\n",
        "        flags = False\n",
        "        for epoch in range(self.n_epoch):\n",
        "            print('Epoch:', epoch + 1)\n",
        "            for train_feed, train_n in self.get_batches(x_train, y_train, batch_size=self.batch_size):\n",
        "                loss_train, acc_train = self.train_on_batch(sess, train_feed)\n",
        "                loss_dev, acc_dev = self.evaluate(sess, x_dev, y_dev)\n",
        "\n",
        "                if total_batch % print_per_batch == 0:\n",
        "                    if acc_dev > best_acc_dev:\n",
        "                        # 保存在验证集上性能最好的模型\n",
        "                        best_acc_dev = acc_dev\n",
        "                        last_improved = total_batch\n",
        "                        if save_dir:\n",
        "                            saver.save(sess=sess, save_path=os.path.join(save_dir, 'sa-model'))\n",
        "                        improved_str = '*'\n",
        "                    else:\n",
        "                        improved_str = ''\n",
        "\n",
        "                    time_dif = time_diff(start_time)\n",
        "                    msg = 'Iter: {0:>6}, 训练集损失: {1:>6.2}, 训练集准确率: {2:>7.2%},' + \\\n",
        "                          ' 验证集损失: {3:>6.2}, 验证集准确率: {4:>7.2%}, 训练时间: {5} {6}'\n",
        "                    print(msg.format(total_batch, loss_train, acc_train, loss_dev, acc_dev, time_dif, improved_str))\n",
        "                total_batch += 1\n",
        "\n",
        "                if total_batch - last_improved > require_improvement:\n",
        "                    print('自动停止...')\n",
        "                    flags = True\n",
        "                    break\n",
        "            if flags:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPqfzP3-JlRb",
        "colab_type": "text"
      },
      "source": [
        "# 4. 模型训练与验证"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JomXp0C-JjIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    config = CONFIG()\n",
        "    print('==============加载word2id=============')\n",
        "    word2id = load_word2id(config.word2id_path)\n",
        "    print('==============加载word2vec=============')\n",
        "    word2vec = load_corpus_word2vec(config.corpus_word2vec_path)\n",
        "    print('==============加载train语料库===========')\n",
        "    x_tr, y_tr = load_corpus(config.train_path, word2id, max_sen_len=config.max_sen_len)\n",
        "    print('==============加载dev语料库=============')\n",
        "    x_val, y_val = load_corpus(config.dev_path, word2id, max_sen_len=config.max_sen_len)\n",
        "    print('==============训练模型=================')\n",
        "    tc = TextCNN(CONFIG, embeddings=word2vec)\n",
        "    with tf.Session() as sess:\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        sess.run(init_op)\n",
        "        tc.fit(sess, x_tr, y_tr, x_val, y_val, config.save_dir, config.print_per_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2PM2sk96dem",
        "colab_type": "code",
        "outputId": "e0e5816e-f77b-4e63-ba53-1b8cc1558fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1435
        }
      },
      "source": [
        "# 模型训练\n",
        "tf.reset_default_graph()\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============加载word2id=============\n",
            "==============加载word2vec=============\n",
            "==============加载train语料库===========\n",
            "总样本数为：19998\n",
            "各个类别样本数如下：\n",
            "1 9999\n",
            "0 9999\n",
            "==============加载dev语料库=============\n",
            "总样本数为：5629\n",
            "各个类别样本数如下：\n",
            "1 2812\n",
            "0 2817\n",
            "==============训练模型=================\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-18-c9f67bd72423>:38: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv1d instead.\n",
            "WARNING:tensorflow:From <ipython-input-18-c9f67bd72423>:46: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-18-c9f67bd72423>:60: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "训练和验证...\n",
            "Epoch: 1\n",
            "Iter:      0, 训练集损失:   0.84, 训练集准确率:  42.00%, 验证集损失:   0.93, 验证集准确率:  50.26%, 训练时间: 0:00:02 *\n",
            "Iter:    100, 训练集损失:   0.57, 训练集准确率:  76.00%, 验证集损失:    0.5, 验证集准确率:  75.95%, 训练时间: 0:01:49 *\n",
            "Epoch: 2\n",
            "Iter:    200, 训练集损失:   0.33, 训练集准确率:  84.00%, 验证集损失:   0.47, 验证集准确率:  78.54%, 训练时间: 0:03:36 *\n",
            "Iter:    300, 训练集损失:   0.36, 训练集准确率:  86.00%, 验证集损失:   0.49, 验证集准确率:  78.34%, 训练时间: 0:05:23 \n",
            "Epoch: 3\n",
            "Iter:    400, 训练集损失:   0.27, 训练集准确率:  86.00%, 验证集损失:   0.41, 验证集准确率:  81.95%, 训练时间: 0:07:10 *\n",
            "Iter:    500, 训练集损失:   0.21, 训练集准确率:  91.00%, 验证集损失:   0.41, 验证集准确率:  83.11%, 训练时间: 0:08:57 *\n",
            "Epoch: 4\n",
            "Iter:    600, 训练集损失:   0.13, 训练集准确率:  97.00%, 验证集损失:    0.4, 验证集准确率:  83.46%, 训练时间: 0:10:44 *\n",
            "Iter:    700, 训练集损失:   0.15, 训练集准确率:  94.00%, 验证集损失:   0.54, 验证集准确率:  81.31%, 训练时间: 0:12:31 \n",
            "Epoch: 5\n",
            "Iter:    800, 训练集损失:  0.023, 训练集准确率: 100.00%, 验证集损失:   0.52, 验证集准确率:  81.97%, 训练时间: 0:14:18 \n",
            "Iter:    900, 训练集损失:  0.085, 训练集准确率:  97.00%, 验证集损失:   0.55, 验证集准确率:  83.51%, 训练时间: 0:16:05 *\n",
            "Epoch: 6\n",
            "Iter:   1000, 训练集损失:  0.016, 训练集准确率: 100.00%, 验证集损失:   0.56, 验证集准确率:  83.44%, 训练时间: 0:17:53 \n",
            "Iter:   1100, 训练集损失:  0.048, 训练集准确率:  99.00%, 验证集损失:   0.65, 验证集准确率:  83.02%, 训练时间: 0:19:40 \n",
            "Epoch: 7\n",
            "Iter:   1200, 训练集损失: 0.0034, 训练集准确率: 100.00%, 验证集损失:   0.68, 验证集准确率:  82.70%, 训练时间: 0:21:27 \n",
            "Iter:   1300, 训练集损失: 0.0028, 训练集准确率: 100.00%, 验证集损失:   0.73, 验证集准确率:  82.77%, 训练时间: 0:23:15 \n",
            "Epoch: 8\n",
            "Iter:   1400, 训练集损失: 0.00083, 训练集准确率: 100.00%, 验证集损失:   0.76, 验证集准确率:  82.86%, 训练时间: 0:25:02 \n",
            "自动停止...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-b4fb66de0ad9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c9f67bd72423>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, sess, x_train, y_train, x_dev, y_dev, save_dir, print_per_batch)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_train_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cnn_train_accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXOMsbmZK0Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    config = CONFIG()\n",
        "    print('===========加载word2id================')\n",
        "    word2id = load_word2id(config.word2id_path)\n",
        "    config.vocab_size = len(word2id)\n",
        "    print('===========加载test语料库==============')\n",
        "    x, y = load_corpus(config.test_path, word2id, max_sen_len=config.max_sen_len)\n",
        "    \n",
        "    model = TextCNN(config)\n",
        "    with tf.Session() as sess:\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        sess.run(init_op)\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.get_checkpoint_state(config.save_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "        yhat = model.predict(sess, x)\n",
        "\n",
        "    cat, cat2id = cat_to_id()\n",
        "    y_cls = np.argmax(y, 1)\n",
        "    # 评估\n",
        "    print(\"Precision, Recall and F1-Score...\")\n",
        "    print(metrics.classification_report(y_cls, yhat, target_names=cat))\n",
        "    # 混淆矩阵\n",
        "    print(\"Confusion Matrix...\")\n",
        "    cm = metrics.confusion_matrix(y_cls, yhat)\n",
        "    print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twNf89MD6jma",
        "colab_type": "code",
        "outputId": "76f98aff-3ff5-43d8-c6bc-9a51f160c409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# 模型测试\n",
        "\n",
        "tf.reset_default_graph() \n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========加载word2id================\n",
            "===========加载test语料库==============\n",
            "总样本数为：369\n",
            "各个类别样本数如下：\n",
            "1 187\n",
            "0 182\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./checkpoints/sa-model\n",
            "Precision, Recall and F1-Score...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       182\n",
            "           1       0.88      0.79      0.83       187\n",
            "\n",
            "    accuracy                           0.84       369\n",
            "   macro avg       0.84      0.84      0.84       369\n",
            "weighted avg       0.84      0.84      0.84       369\n",
            "\n",
            "Confusion Matrix...\n",
            "[[161  21]\n",
            " [ 39 148]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN_gPiytJ-DP",
        "colab_type": "text"
      },
      "source": [
        "# 5. 模型预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWC2ZrGPUyHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_id(inputs):\n",
        "    \"\"\"\n",
        "    将语句进行分词，然后将词语转换为word_to_id中的id编码\n",
        "    :param inputs: 句子：列表的形式\n",
        "    :return: 用id表征的语句\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    cut_sents = [jb.cut(w) for w in inputs]\n",
        "    config = CONFIG()\n",
        "    word2id = load_word2id(config.word2id_path)\n",
        "\n",
        "    for cut_sent in cut_sents:\n",
        "        sentence = [word2id.get(w, 0) for w in cut_sent]\n",
        "        sentence = sentence[:config.max_sen_len]\n",
        "        if len(sentence) < config.max_sen_len:\n",
        "            sentence += [word2id['_PAD_']] * (config.max_sen_len - len(sentence))\n",
        "\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return np.asarray(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x2C3aS_5dpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(x, label=False, prob=False):\n",
        "    \"\"\"\n",
        "    :param x: 语句列表\n",
        "    :param label: 是否以分类标签的形式：pos或neg输出。默认为：0/1\n",
        "    :param prob: 是否以概率的形式输出。\n",
        "    :return: 情感预测结果\n",
        "    \"\"\"\n",
        "    if label and prob:\n",
        "        raise Exception(\"label和prob两个参数不能同时为True!\")\n",
        "\n",
        "    x = sent_to_id(x)\n",
        "    config = CONFIG()\n",
        "    model = TextCNN(config)\n",
        "    with tf.Session() as sess:\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        sess.run(init_op)\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.get_checkpoint_state(config.save_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "        y = model.predict(sess, x, prob=prob)\n",
        "\n",
        "    if label:\n",
        "        cat, _ = cat_to_id()\n",
        "        y = [cat[w] for w in y.tolist()]\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXDf_Z3i69AJ",
        "colab_type": "code",
        "outputId": "a0f35d85-cec2-4f19-ceeb-1f35ca7b4ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3101
        }
      },
      "source": [
        "# 使用训练所得模型进行电影评论分析 label :0--pos /1--neg\n",
        "\n",
        "tf.reset_default_graph()\n",
        "test = ['完成度很高的公路喜剧片，亮点在于人物塑造完整和细节使用精彩，剧作与表演堪称杰出。','爆米花电影，特效不错，可以一看。但也有几处硬伤']\n",
        "print(predict(test, label=False, prob=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./checkpoints/sa-model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [59290,50] rhs shape= [58954,50]\n\t [[{{node save/Assign}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [59290,50] rhs shape= [58954,50]\n\t [[node save/Assign (defined at <ipython-input-24-ecd90fedb4b6>:17) ]]\n\nCaused by op 'save/Assign', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-2d149a448c5b>\", line 4, in <module>\n    print(predict(test, label=False, prob=True))\n  File \"<ipython-input-24-ecd90fedb4b6>\", line 17, in predict\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [59290,50] rhs shape= [58954,50]\n\t [[node save/Assign (defined at <ipython-input-24-ecd90fedb4b6>:17) ]]\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2d149a448c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'完成度很高的公路喜剧片，亮点在于人物塑造完整和细节使用精彩，剧作与表演堪称杰出。'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'爆米花电影，特效不错，可以一看。但也有几处硬伤'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-ecd90fedb4b6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, label, prob)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1312\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [59290,50] rhs shape= [58954,50]\n\t [[node save/Assign (defined at <ipython-input-24-ecd90fedb4b6>:17) ]]\n\nCaused by op 'save/Assign', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-2d149a448c5b>\", line 4, in <module>\n    print(predict(test, label=False, prob=True))\n  File \"<ipython-input-24-ecd90fedb4b6>\", line 17, in predict\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [59290,50] rhs shape= [58954,50]\n\t [[node save/Assign (defined at <ipython-input-24-ecd90fedb4b6>:17) ]]\n"
          ]
        }
      ]
    }
  ]
}