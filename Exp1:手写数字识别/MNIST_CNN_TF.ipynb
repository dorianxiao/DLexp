{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorianxiao/DLexp/blob/master/Exp1%3A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/MNIST_CNN_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NQgIkd2cgAh",
        "colab_type": "text"
      },
      "source": [
        "# 1. 初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaPmU0kiaR92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入相关库\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxAVRqOAf9sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义全局变量\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "NUM_CHANNELS = 1\n",
        "PIXEL_DEPTH = 255\n",
        "NUM_LABELS = 10\n",
        "VALIDATION_SIZE = 5000    # 验证集大小\n",
        "SEED = 66478\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "EVAL_BATCH_SIZE = 64\n",
        "EVAL_FREQUENCY = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hWE0iS_cvqC",
        "colab_type": "text"
      },
      "source": [
        "# 2. 下载数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVTxZsX6eDwZ",
        "colab_type": "text"
      },
      "source": [
        "### 挂载到Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmYKwqSdeLDR",
        "colab_type": "code",
        "outputId": "850462c4-9f3b-4609-fa23-cfacb04d2f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLX_7nVcpJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(filename):\n",
        "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
        "  \n",
        "  # 如果没有下载\n",
        "  if not tf.gfile.Exists(filepath):\n",
        "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
        "    with tf.gfile.GFile(filepath) as f:\n",
        "      size = f.size()\n",
        "    print('成功下载 ', filename, size, 'bytes.')\n",
        "  else:\n",
        "    print('目录中已存在 ', filename)\n",
        "  return filepath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8FeAO90c4vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 指定下载目录\n",
        "\n",
        "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
        "WORK_DIRECTORY = r'/content/gdrive/My Drive/mylab'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9eJF_esebEm",
        "colab_type": "code",
        "outputId": "9d4ab9e1-2448-47d2-c51c-b00b804e3b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# 开始下载\n",
        "\n",
        "train_data_filename = download('train-images-idx3-ubyte.gz')\n",
        "train_labels_filename = download('train-labels-idx1-ubyte.gz')\n",
        "test_data_filename = download('t10k-images-idx3-ubyte.gz')\n",
        "test_labels_filename = download('t10k-labels-idx1-ubyte.gz')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "成功下载  train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "成功下载  train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "成功下载  t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "成功下载  t10k-labels-idx1-ubyte.gz 4542 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyEFiQQce27i",
        "colab_type": "code",
        "outputId": "3aa94df8-62e8-4f2c-a478-c7b9a4e24432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 查看是否下载成功\n",
        "\n",
        "!ls /content/gdrive/My\\ Drive/mylab"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Rbpy9XgaUd",
        "colab_type": "text"
      },
      "source": [
        "# 3. 加载数据\n",
        "## 加载图像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWhaOl0Le_fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_data(filename, num_images):\n",
        "  \"\"\"把图片加载到4维张量[image index, y, x, channels].\n",
        "\n",
        "  像素值[0, 255]被调整到[-0.5, 0.5].\n",
        "  \"\"\"\n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(16)  #每个像素存储在文件中的大小为16bits\n",
        "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
        "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
        "    \n",
        "    # 像素值[0, 255]被调整到[-0.5, 0.5]\n",
        "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
        "    \n",
        "    # reshape成4维张量[image index, y, x, channels]\n",
        "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDSj0ytQiMxo",
        "colab_type": "text"
      },
      "source": [
        "## 加载标签"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxzY4wVCiDDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_labels(filename, num_images):\n",
        "  \"\"\"把标签加载到int64型的标签向量.\"\"\"\n",
        "  \n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(8)     #每个标签存储在文件中的大小为8bits\n",
        "    buf = bytestream.read(1 * num_images)\n",
        "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3002xtuiqwG",
        "colab_type": "code",
        "outputId": "3c186df2-b4d3-41e5-a49a-3043d252ed75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# 加载数据到numpy数组\n",
        "\n",
        "train_data = extract_data(train_data_filename, 60000)\n",
        "train_labels = extract_labels(train_labels_filename, 60000)\n",
        "test_data = extract_data(test_data_filename, 10000)\n",
        "test_labels = extract_labels(test_labels_filename, 10000)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/gdrive/My Drive/mylab/train-images-idx3-ubyte.gz\n",
            "Extracting /content/gdrive/My Drive/mylab/train-labels-idx1-ubyte.gz\n",
            "Extracting /content/gdrive/My Drive/mylab/t10k-images-idx3-ubyte.gz\n",
            "Extracting /content/gdrive/My Drive/mylab/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuRjUTzdjVc2",
        "colab_type": "text"
      },
      "source": [
        "# 4. 生成验证集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuD_2BCEiy_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
        "validation_labels = train_labels[:VALIDATION_SIZE]\n",
        "train_data = train_data[VALIDATION_SIZE:, ...]\n",
        "train_labels = train_labels[VALIDATION_SIZE:]\n",
        "num_epochs = NUM_EPOCHS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJV5rU1um5tx",
        "colab_type": "code",
        "outputId": "766fd528-b7ae-450e-f7a0-ebfbb4a1354a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 查看训练集和验证集大小\n",
        "\n",
        "train_size = train_labels.shape[0]\n",
        "val_size = validation_labels.shape[0]\n",
        "\n",
        "print(\"训练集大小： \", train_size)\n",
        "print(\"验证集大小： \", val_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练集大小：  55000\n",
            "验证集大小：  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBZWY5X5oj4e",
        "colab_type": "text"
      },
      "source": [
        "# 5. 构建模型计算图\n",
        "## 创建输入占位符"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkgA3CeTnIoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_node = tf.placeholder(\n",
        "    tf.float32,\n",
        "    shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "\n",
        "train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
        "\n",
        "eval_data = tf.placeholder(\n",
        "    tf.float32,\n",
        "    shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XzzWGLpp1cx",
        "colab_type": "text"
      },
      "source": [
        "## 初始化变量\n",
        "调用tf.global_variables_initializer().run()时生效\n",
        "![本次实验复现的卷积网络](https://pic4.zhimg.com/v2-296cbc69aa81f9ed5b56a912d2ab5c70_1200x500.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS1B-ILep0OK",
        "colab_type": "code",
        "outputId": "de1f25a0-165f-41b3-a121-81e81da08bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 初始化卷积层1权重满足标准正态分布\n",
        "# 卷积核用4维张量表示[fliter_height,filter_width,in_channels,out_channels]\n",
        "conv1_weights = tf.Variable(\n",
        "    tf.truncated_normal([5, 5, NUM_CHANNELS, 32],# 5x5 卷积核,  32个卷积核.\n",
        "                        stddev=0.1,\n",
        "                        seed=SEED, dtype=tf.float32))\n",
        "# 初始化卷积层1的偏置\n",
        "conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "# 初始化卷积层2权重\n",
        "conv2_weights = tf.Variable(tf.truncated_normal(\n",
        "    [5, 5, 32, 64], stddev=0.1,\n",
        "    seed=SEED, dtype=tf.float32))\n",
        "# 初始化卷积层2的偏置\n",
        "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=tf.float32))\n",
        "\n",
        "# 初始化全连接层1权重\n",
        "fc1_weights = tf.Variable(  # 全连接层, depth 512.\n",
        "    tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
        "                        stddev=0.1,\n",
        "                        seed=SEED,\n",
        "                        dtype=tf.float32))\n",
        "# 初始化全连接层1偏置\n",
        "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=tf.float32))\n",
        "\n",
        "# 初始化全连接层2权重\n",
        "fc2_weights = tf.Variable(\n",
        "    tf.truncated_normal([512, NUM_LABELS],\n",
        "                        stddev=0.1,seed=SEED,dtype=tf.float32))\n",
        "# 初始化全连接层2偏置(最终输出10维)\n",
        "fc2_biases = tf.Variable(tf.constant(\n",
        "    0.1, shape=[NUM_LABELS], dtype=tf.float32))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkNqBUvf2nUe",
        "colab_type": "text"
      },
      "source": [
        "## 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0yGV8rTpu6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(data, train=False):\n",
        "  \"\"\"构建模型.\"\"\"\n",
        "  # 2维卷积用'SAME'填充 (输出feature map与输入相同) has\n",
        "  # strides是一个4维数组: [image index, y, x, depth].\n",
        "  \n",
        "  ###################\n",
        "  # 进行第一次卷积\n",
        "  ###################\n",
        "  conv = tf.nn.conv2d(data,\n",
        "                      conv1_weights,\n",
        "                      strides=[1, 1, 1, 1],\n",
        "                      padding='SAME')\n",
        "  \n",
        "  # 偏置和ReLU非线性激活.\n",
        "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
        "  \n",
        "  ###################\n",
        "  # 进行第一次池化(最大池化)\n",
        "  ###################\n",
        "  # ksize 池化窗口尺寸为2，一般第一个和最后一个参数固定为1\n",
        "  # strides 池化窗口移动步幅也为2.\n",
        "  pool = tf.nn.max_pool(relu,\n",
        "                        ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1],\n",
        "                        padding='SAME')\n",
        "  \n",
        "  ###################\n",
        "  # 进行第二次卷积\n",
        "  ###################\n",
        "  conv = tf.nn.conv2d(pool,\n",
        "                      conv2_weights,\n",
        "                      strides=[1, 1, 1, 1],\n",
        "                      padding='SAME')\n",
        "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
        "  \n",
        "  ###################\n",
        "  # 进行第二次池化(最大池化)\n",
        "  ###################\n",
        "  pool = tf.nn.max_pool(relu,\n",
        "                        ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1],\n",
        "                        padding='SAME')\n",
        "\n",
        "  # 将feature map变换为2维矩阵，提供给全连接层\n",
        "  pool_shape = pool.get_shape().as_list()  # 转化为list\n",
        "  reshape = tf.reshape(pool,\n",
        "      [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
        "  \n",
        "  # 全连接层\n",
        "  hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
        "  \n",
        "  # 训练时引入dropout防止过拟合\n",
        "  if train:\n",
        "    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
        "  return tf.matmul(hidden, fc2_weights) + fc2_biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmdrHRxvF7xM",
        "colab_type": "text"
      },
      "source": [
        "## 训练与评估\n",
        "### 损失计算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgwmxjtVF0D9",
        "colab_type": "code",
        "outputId": "5deb60c2-5418-4421-d997-aeb073cd64e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 交叉熵损失\n",
        "logits = model(train_data_node, True)\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "    labels=train_labels_node, logits=logits))\n",
        "\n",
        "# 全连接层的L2正则化损失\n",
        "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
        "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
        "# 求得最后损失\n",
        "loss += 5e-4 * regularizers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-5175dfd64eda>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHuTYA_1JMi9",
        "colab_type": "text"
      },
      "source": [
        "### 优化器优化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLLgx3eSI03Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 优化器（用于参数更新）\n",
        "# 设置一个每批增加一次的变量，来控制学习率衰减\n",
        "batch = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "# 每个epoch衰减一次，使用从0.01开始的指数衰减\n",
        "learning_rate = tf.train.exponential_decay(\n",
        "  0.01,                # 初始学习率.\n",
        "  batch * BATCH_SIZE,  # 当前索引.\n",
        "  train_size,          # 衰减步数.\n",
        "  0.95,                # 衰减率.\n",
        "  staircase=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXIYhFQbM1xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 用momentum优化器优化\n",
        "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9t_4SS5NDvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # 预测当前的训练批\n",
        "  train_prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8HGQl5WQZqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 预测验证批\n",
        "eval_prediction = tf.nn.softmax(model(eval_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ev29SikQh0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_in_batches(data, sess):\n",
        "  \"\"\"预测所有.\"\"\"\n",
        "  size = data.shape[0]\n",
        "  if size < EVAL_BATCH_SIZE:\n",
        "    raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
        "  predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
        "  for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
        "    end = begin + EVAL_BATCH_SIZE\n",
        "    if end <= size:\n",
        "      predictions[begin:end, :] = sess.run(\n",
        "          eval_prediction,\n",
        "          feed_dict={eval_data: data[begin:end, ...]})\n",
        "    else:\n",
        "      batch_predictions = sess.run(\n",
        "          eval_prediction,\n",
        "          feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
        "      predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF7RYqUnRFCx",
        "colab_type": "text"
      },
      "source": [
        "# 创建会话"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUGthlOSXkLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义错误率函数\n",
        "def error_rate(predictions, labels):\n",
        "  return 100.0 - (\n",
        "      100.0 *\n",
        "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
        "      predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-J2QMa9Q-_Q",
        "colab_type": "code",
        "outputId": "1d26d794-846b-4e96-a02c-a8911f4cc6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5899
        }
      },
      "source": [
        "# 创建会话进行训练\n",
        "start_time = time.time()\n",
        "with tf.Session() as sess:\n",
        "  # 初始化可训练变量\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('初始化完成！')\n",
        "  \n",
        "  # 训练循环开始\n",
        "  for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
        "    \n",
        "    # 计算当前批的偏置\n",
        "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
        "    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
        "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
        "    \n",
        "    # 将数据喂入计算图\n",
        "    feed_dict = {train_data_node: batch_data,\n",
        "                 train_labels_node: batch_labels}\n",
        "    \n",
        "    # 运行优化器更新参数\n",
        "    sess.run(optimizer, feed_dict=feed_dict)\n",
        "    \n",
        "    # 每隔100步打印相关信息\n",
        "    if step % EVAL_FREQUENCY == 0:\n",
        "      # fetch some extra nodes' data\n",
        "      l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n",
        "                                    feed_dict=feed_dict)\n",
        "      elapsed_time = time.time() - start_time\n",
        "      start_time = time.time()\n",
        "      print('Step %d (epoch %.2f), %.1f ms' %\n",
        "            (step, float(step) * BATCH_SIZE / train_size,\n",
        "             1000 * elapsed_time / EVAL_FREQUENCY))\n",
        "      print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
        "      print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
        "      print('Validation error: %.1f%%' % error_rate(\n",
        "          eval_in_batches(validation_data, sess), validation_labels))\n",
        "      sys.stdout.flush()\n",
        "      \n",
        "  # 打印最后的结果\n",
        "  test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
        "  print('Test error: %.1f%%' % test_error)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "初始化完成！\n",
            "Step 0 (epoch 0.00), 7.5 ms\n",
            "Minibatch loss: 8.334, learning rate: 0.010000\n",
            "Minibatch error: 85.9%\n",
            "Validation error: 84.6%\n",
            "Step 100 (epoch 0.12), 212.7 ms\n",
            "Minibatch loss: 3.238, learning rate: 0.010000\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 7.3%\n",
            "Step 200 (epoch 0.23), 232.8 ms\n",
            "Minibatch loss: 3.352, learning rate: 0.010000\n",
            "Minibatch error: 10.9%\n",
            "Validation error: 4.2%\n",
            "Step 300 (epoch 0.35), 224.3 ms\n",
            "Minibatch loss: 3.139, learning rate: 0.010000\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 3.0%\n",
            "Step 400 (epoch 0.47), 239.2 ms\n",
            "Minibatch loss: 3.212, learning rate: 0.010000\n",
            "Minibatch error: 9.4%\n",
            "Validation error: 2.9%\n",
            "Step 500 (epoch 0.58), 291.4 ms\n",
            "Minibatch loss: 3.177, learning rate: 0.010000\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 2.5%\n",
            "Step 600 (epoch 0.70), 395.9 ms\n",
            "Minibatch loss: 3.114, learning rate: 0.010000\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 2.1%\n",
            "Step 700 (epoch 0.81), 384.0 ms\n",
            "Minibatch loss: 2.976, learning rate: 0.010000\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 2.2%\n",
            "Step 800 (epoch 0.93), 394.9 ms\n",
            "Minibatch loss: 3.042, learning rate: 0.010000\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 2.3%\n",
            "Step 900 (epoch 1.05), 397.1 ms\n",
            "Minibatch loss: 2.906, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.6%\n",
            "Step 1000 (epoch 1.16), 224.6 ms\n",
            "Minibatch loss: 2.872, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.8%\n",
            "Step 1100 (epoch 1.28), 222.5 ms\n",
            "Minibatch loss: 2.837, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.5%\n",
            "Step 1200 (epoch 1.40), 224.1 ms\n",
            "Minibatch loss: 2.961, learning rate: 0.009500\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.5%\n",
            "Step 1300 (epoch 1.51), 225.9 ms\n",
            "Minibatch loss: 2.802, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.6%\n",
            "Step 1400 (epoch 1.63), 225.1 ms\n",
            "Minibatch loss: 2.801, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.5%\n",
            "Step 1500 (epoch 1.75), 225.9 ms\n",
            "Minibatch loss: 2.860, learning rate: 0.009500\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 1.4%\n",
            "Step 1600 (epoch 1.86), 226.8 ms\n",
            "Minibatch loss: 2.738, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.4%\n",
            "Step 1700 (epoch 1.98), 228.3 ms\n",
            "Minibatch loss: 2.659, learning rate: 0.009500\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.6%\n",
            "Step 1800 (epoch 2.09), 225.3 ms\n",
            "Minibatch loss: 2.678, learning rate: 0.009025\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.3%\n",
            "Step 1900 (epoch 2.21), 223.0 ms\n",
            "Minibatch loss: 2.634, learning rate: 0.009025\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 2000 (epoch 2.33), 223.8 ms\n",
            "Minibatch loss: 2.610, learning rate: 0.009025\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 2100 (epoch 2.44), 228.5 ms\n",
            "Minibatch loss: 2.589, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 2200 (epoch 2.56), 224.0 ms\n",
            "Minibatch loss: 2.563, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 2300 (epoch 2.68), 225.2 ms\n",
            "Minibatch loss: 2.599, learning rate: 0.009025\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.2%\n",
            "Step 2400 (epoch 2.79), 221.7 ms\n",
            "Minibatch loss: 2.513, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 2500 (epoch 2.91), 222.7 ms\n",
            "Minibatch loss: 2.472, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 2600 (epoch 3.03), 223.6 ms\n",
            "Minibatch loss: 2.457, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 2700 (epoch 3.14), 230.8 ms\n",
            "Minibatch loss: 2.489, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.1%\n",
            "Step 2800 (epoch 3.26), 224.9 ms\n",
            "Minibatch loss: 2.535, learning rate: 0.008574\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.3%\n",
            "Step 2900 (epoch 3.37), 220.6 ms\n",
            "Minibatch loss: 2.511, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.1%\n",
            "Step 3000 (epoch 3.49), 221.4 ms\n",
            "Minibatch loss: 2.403, learning rate: 0.008574\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 0.9%\n",
            "Step 3100 (epoch 3.61), 223.6 ms\n",
            "Minibatch loss: 2.356, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3200 (epoch 3.72), 221.7 ms\n",
            "Minibatch loss: 2.331, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 3300 (epoch 3.84), 221.2 ms\n",
            "Minibatch loss: 2.313, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3400 (epoch 3.96), 220.3 ms\n",
            "Minibatch loss: 2.294, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3500 (epoch 4.07), 220.7 ms\n",
            "Minibatch loss: 2.287, learning rate: 0.008145\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 3600 (epoch 4.19), 219.3 ms\n",
            "Minibatch loss: 2.252, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3700 (epoch 4.31), 219.9 ms\n",
            "Minibatch loss: 2.228, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 3800 (epoch 4.42), 218.6 ms\n",
            "Minibatch loss: 2.221, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 3900 (epoch 4.54), 218.7 ms\n",
            "Minibatch loss: 2.230, learning rate: 0.008145\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 4000 (epoch 4.65), 219.3 ms\n",
            "Minibatch loss: 2.252, learning rate: 0.008145\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.1%\n",
            "Step 4100 (epoch 4.77), 218.3 ms\n",
            "Minibatch loss: 2.171, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 4200 (epoch 4.89), 218.1 ms\n",
            "Minibatch loss: 2.169, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 4300 (epoch 5.00), 219.0 ms\n",
            "Minibatch loss: 2.190, learning rate: 0.007738\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.1%\n",
            "Step 4400 (epoch 5.12), 218.4 ms\n",
            "Minibatch loss: 2.147, learning rate: 0.007738\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.1%\n",
            "Step 4500 (epoch 5.24), 222.6 ms\n",
            "Minibatch loss: 2.184, learning rate: 0.007738\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 0.9%\n",
            "Step 4600 (epoch 5.35), 218.5 ms\n",
            "Minibatch loss: 2.083, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 4700 (epoch 5.47), 219.0 ms\n",
            "Minibatch loss: 2.079, learning rate: 0.007738\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 4800 (epoch 5.59), 218.9 ms\n",
            "Minibatch loss: 2.052, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 4900 (epoch 5.70), 217.7 ms\n",
            "Minibatch loss: 2.051, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5000 (epoch 5.82), 218.2 ms\n",
            "Minibatch loss: 2.133, learning rate: 0.007738\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 0.9%\n",
            "Step 5100 (epoch 5.93), 222.8 ms\n",
            "Minibatch loss: 2.006, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 5200 (epoch 6.05), 223.9 ms\n",
            "Minibatch loss: 2.101, learning rate: 0.007351\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 0.9%\n",
            "Step 5300 (epoch 6.17), 223.9 ms\n",
            "Minibatch loss: 1.973, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5400 (epoch 6.28), 226.6 ms\n",
            "Minibatch loss: 1.962, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5500 (epoch 6.40), 221.7 ms\n",
            "Minibatch loss: 1.988, learning rate: 0.007351\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 5600 (epoch 6.52), 221.6 ms\n",
            "Minibatch loss: 1.930, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5700 (epoch 6.63), 223.2 ms\n",
            "Minibatch loss: 1.913, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5800 (epoch 6.75), 224.3 ms\n",
            "Minibatch loss: 1.899, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5900 (epoch 6.87), 226.4 ms\n",
            "Minibatch loss: 1.892, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6000 (epoch 6.98), 222.0 ms\n",
            "Minibatch loss: 1.888, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6100 (epoch 7.10), 222.3 ms\n",
            "Minibatch loss: 1.862, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6200 (epoch 7.21), 223.5 ms\n",
            "Minibatch loss: 1.843, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6300 (epoch 7.33), 223.1 ms\n",
            "Minibatch loss: 1.840, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 6400 (epoch 7.45), 223.1 ms\n",
            "Minibatch loss: 1.845, learning rate: 0.006983\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 6500 (epoch 7.56), 223.8 ms\n",
            "Minibatch loss: 1.806, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 6600 (epoch 7.68), 222.8 ms\n",
            "Minibatch loss: 1.810, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6700 (epoch 7.80), 223.8 ms\n",
            "Minibatch loss: 1.786, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 6800 (epoch 7.91), 224.5 ms\n",
            "Minibatch loss: 1.769, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 6900 (epoch 8.03), 223.7 ms\n",
            "Minibatch loss: 1.757, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7000 (epoch 8.15), 223.7 ms\n",
            "Minibatch loss: 1.750, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 7100 (epoch 8.26), 224.1 ms\n",
            "Minibatch loss: 1.734, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7200 (epoch 8.38), 225.2 ms\n",
            "Minibatch loss: 1.736, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 7300 (epoch 8.49), 225.9 ms\n",
            "Minibatch loss: 1.763, learning rate: 0.006634\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 0.8%\n",
            "Step 7400 (epoch 8.61), 222.2 ms\n",
            "Minibatch loss: 1.701, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7500 (epoch 8.73), 222.3 ms\n",
            "Minibatch loss: 1.694, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7600 (epoch 8.84), 222.5 ms\n",
            "Minibatch loss: 1.762, learning rate: 0.006634\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.8%\n",
            "Step 7700 (epoch 8.96), 222.6 ms\n",
            "Minibatch loss: 1.667, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 7800 (epoch 9.08), 221.3 ms\n",
            "Minibatch loss: 1.657, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7900 (epoch 9.19), 221.3 ms\n",
            "Minibatch loss: 1.646, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8000 (epoch 9.31), 221.0 ms\n",
            "Minibatch loss: 1.645, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8100 (epoch 9.43), 220.2 ms\n",
            "Minibatch loss: 1.628, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8200 (epoch 9.54), 218.5 ms\n",
            "Minibatch loss: 1.620, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8300 (epoch 9.66), 219.2 ms\n",
            "Minibatch loss: 1.610, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8400 (epoch 9.77), 219.6 ms\n",
            "Minibatch loss: 1.595, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.7%\n",
            "Step 8500 (epoch 9.89), 220.9 ms\n",
            "Minibatch loss: 1.613, learning rate: 0.006302\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Test error: 0.8%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}